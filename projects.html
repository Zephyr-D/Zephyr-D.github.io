<!-- 
  Author: Li Ding
  2020/01/23
-->

<!DOCTYPE html>
<html style="height: 100%;">

<head>
  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Li Ding | 丁立</title>
  <meta name="description" content="Li Ding's personal website.">
  <meta name="author" content="Li Ding">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Raleway:400,300,600">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.7.2/css/all.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- <link rel="icon" type="image/png" href="images/favicon.png">-->


<body>
  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <header class="container">
  <br>
  <nav class="navbar">
    <ul class="navbar-list">
      <li class="navbar-item">
        <a class="navbar-link" href=".">Li Ding | 丁立</a>
      </li>
      <li class="navbar-item">
        <a class="navbar-link active" href="projects.html">Projects</a>
      </li>
      <li class="navbar-item">
        <a class="navbar-link" href="publications.html">Publications</a>
      </li>
      <li class="navbar-item">
        <a class="navbar-link" href="misc.html">Misc.</a>
      </li>
    </ul>
  </nav>
</header>

<div class="container">
  <hr>
  <!-- Research  
    –––––––––––––––––––––––––––––––––––––––––––––––––– -->

  <!-- 2019  
      –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <h6 style="color:dimgray; margin-left: 2em">
    <b>2019</b>
  </h6>

  <p align="center">
    <b>Semantic Driving Scene Generation and Diagnosis</b>
  </p>
  <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
    <div class="six columns">
      <p>
        Human perception forms different levels of abstractions covering the
        essential semantic components. Based on this intuition, we propose a
        multi-scale representation built on the top of semantic images, and
        further show how such representation can be effectively used for various
        tasks, including perceptive evaluation of semantic segmentation, semantic
        scene generation, and anomaly detection in segmentation predictions.
        <br>
        Paper under review, arXiv preprint will be out soon.
      </p>
    </div>
    <div class="six columns">
      <img src="images/proj_gan.jpg" style="width:95%">
    </div>
  </div>
  <br>

  <p align="center">
    <b>Distribution Representation for Object Detection</b>
  </p>
  <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
    <div class="six columns">
      <img src="images/pub_object.jpg" style="width:95%">
    </div>
    <div class="six columns">
      <p>
        Object detection has been a critical part of visual scene understanding.
        The representation of the object has important implications for algorithm
        design. In this work, we propose a new representation of objects based on
        the bivariate normal distribution, which has the benefit of robust
        detection of highly-overlapping objects.
        <br>
        [<a href="materials/1907.12929.pdf" target="_blank">pdf</a>]
        [<a href="https://arxiv.org/abs/1907.12929" target="_blank">arXiv</a>]
      </p>
    </div>
  </div>
  <br>

  <p align="center">
    <b>Cognitive Load Estimation with Pupil Movement Detection</b>
  </p>
  <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
    <div class="six columns">
      <p>
        Cognitive load has been shown to be an important variable for
        understanding human performance. We propose novel deep learning
        architectures for joint blink, pupil, and eye landmarks detection, and
        explore different methods of using dynamic pupil movements to estimate
        human cognitive load.
        <br>
        Paper and dataset release in progress.
      </p>
    </div>
    <div class="six columns">
      <img src="images/proj_cog.jpg" style="width:95%">
    </div>
  </div>
  <br>

  <!-- 2018  
      –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <br>
  <h6 style="color:dimgray; margin-left: 2em">
    <b>2018</b>
  </h6>

  <p align="center">
    <b>Black Betty: MIT Human-Centered Autonomous Vehicle</b>
  </p>
  <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
    <div class="six columns">
      <video width="95%" autoplay loop muted>
        <source src="images/proj_hcav.mp4" type="video/mp4">
      </video>
    </div>
    <div class="six columns">
      <p>
        The interaction between human and machine with growing intelligence
        challenges our assumptions about the limitations of human beings at their
        worst and the capabilities of AI systems at their best. We explore
        Human-Centered Autonomous vehicle as an illustrative case study of
        concepts in shared autonomy.<br>
        [<a href="https://hcai.mit.edu/hcav/" target="_blank">project
          page</a>]
      </p>
    </div>
  </div>
  <br>

  <p align="center">
    <b>Arguing Machines: Human Supervision of Black Box AI Systems</b>
  </p>
  <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
    <div class="six columns">
      <p>
        We consider the paradigm of a black box AI system that makes life-critical
        decisions. By proposing an“arguing machines” framework, we show that the
        disagreement between the two AI systems, without any knowledge of
        underlying system design or operation, is sufficient to arbitrarily
        improve the accuracy of the overall decision pipeline if given human
        supervision over disagreements. <br>
        Published in CVPR 2019: Workshop on Autonomous Driving.<br>
        [<a href="materials/fridman2019arguing.pdf" target="_blank">pdf</a>]
        [<a href="https://arxiv.org/abs/1710.04459" target="_blank">arXiv</a>]
        [<a href="https://hcai.mit.edu/arguing-machines/" target="_blank">project
          page</a>]
      </p>
    </div>
    <div class="six columns">
      <img src="images/proj_argue.jpg" style="width:95%">
    </div>
  </div>
  <br>

  <!--
    <p align="center">
      <b>Switch of Vehicle Control in Racing Simulation</b>
    </p>
    <div class="row" style="display: flex; align-items: center">
      <div class="six columns">
        <video width="95%" autoplay loop muted>
          <source src="images/proj_e2e.mp4" type="video/mp4">
        </video>
      </div>
      <div class="six columns">
        <p>
          A racing simulation environment with fully functioned end-to-end autonomous driving system is developed
          based on
          <i>Forza Motorsport 7</i>.
          We study the fundamental behavior in collaborative vehicle control between human players and autonomous
          driving algorithms.
        </p>
      </div>
    </div>
    <br>
  -->

  <p align="center">
    <b>Dynamic Driving Scene Segmentation and Large-Scale Annotation</b>
  </p>
  <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
    <div class="six columns">
      <video width="95%" style="margin-top: 0em" autoplay loop muted>
        <source src="images/proj_seg.mp4" type="video/mp4">
      </video>
    </div>
    <div class="six columns">
      <p>
        We leverage the latest semantic segmentation techniques for the task of
        driving scene segmentation, and propose a novel deep learning approach to
        extract spatio-temporal context that helps improve model performance. We
        also invest in large-scale image annotation process for datasets, and
        develop methods in semi-automated annotation. <br>
        Paper under review. <br>
        [<a href="materials/1904.00758.pdf" target="_blank">pdf</a>]
        [<a href="https://arxiv.org/abs/1904.00758" target="_blank">arXiv</a>]
        <!-- and on <a href="https://selfdrivingcars.mit.edu/segfuse" target="_blank">project page</a>.-->
      </p>
    </div>
  </div>
  <br>

  <!-- 2017  
      –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <br>
  <h6 style="color:dimgray; margin-left: 2em">
    <b>2017</b>
  </h6>

  <p align="center">
    <b>Fully / Weakly Supervised Action Localization</b>
  </p>
  <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
    <div class="six columns">
      <p>
        We propose a new action classification model and a novel iterative
        alignment approach to address weakly supervised action localization task
        that only require the transcript to learn the exact time of actions. <br>
        Published in CVPR 2018. <br>
        [<a href="materials/1803.10699.pdf" target="_blank">pdf</a>]
        [<a href="https://arxiv.org/abs/1803.10699" target="_blank">arXiv</a>]
        [<a href="https://github.com/Zephyr-D/TCFPN-ISBA" target="_blank">code</a>]
        [<a href="materials/ding2018weakly_poster.pdf" target="_blank">poster</a>]
      </p>
    </div>
    <div class="six columns">
      <img src="images/proj_act.jpg" style="width:95%">
    </div>
  </div>
  <br>
</div>

  <!-- Footer  
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <footer>
    <div class="container">
      <hr>
      <p align="center" style="margin-top: -3rem; margin-bottom: 1rem;">
        <small>Copyright © 2017-2020 Ding, Li</small>
      </p>
    </div>
  </footer>

  <!-- End Document
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>

</html>